{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TLDR\n",
    "\n",
    "This notebook explores running preferential BO on the shapes dataset. \n",
    "\n",
    "## Summary of the PBO procedure:\n",
    "- create outcome function (Image) and utility function (area, or fancier versions)\n",
    "- generate initial data of (image 1, image 2, preference)\n",
    "- train a utility model\n",
    "- iteratively use EUBO to generate new comparisons, expand data, refit utility model; generate image that maximizes utility; plot the utility of the candidate over comparisons\n",
    "\n",
    "Caveats:\n",
    "- bounds\n",
    "\n",
    "Ryan's notebook: https://github.com/zyyjjj/low_rank_BOPE/blob/ryan-dev/notebooks/GitHub%20PCA-BO%20for%20Shapes%20Dataset.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import re\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from dataclasses import asdict, dataclass\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "import torch \n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('/home/yz685/low_rank_BOPE')\n",
    "sys.path.append('/home/yz685/low_rank_BOPE/low_rank_BOPE')\n",
    "\n",
    "from botorch.models import PairwiseGP, PairwiseLaplaceMarginalLogLikelihood\n",
    "from botorch.models.transforms.input import ChainedInputTransform\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.acquisition.preference import AnalyticExpectedUtilityOfBestOption\n",
    "from botorch.optim.optimize import optimize_acqf\n",
    "\n",
    "\n",
    "\n",
    "from low_rank_BOPE.bope_class import BopeExperiment\n",
    "from low_rank_BOPE.test_problems.shapes import AreaUtil, LargestRectangleUtil, GradientAwareAreaUtil, Image\n",
    "from low_rank_BOPE.src.transforms import InputCenter, PCAInputTransform\n",
    "from low_rank_BOPE.src.pref_learning_helpers import gen_comps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BopeExperiment settings:  {'pca_var_threshold': 0.9, 'initial_experimentation_batch': 16, 'n_check_post_mean': 20, 'every_n_comps': 3, 'verbose': True, 'dtype': torch.float64, 'noise_std': 0.01, 'num_restarts': 20, 'raw_samples': 128, 'batch_limit': 4, 'sampler_num_outcome_samples': 64, 'maxiter': 1000, 'latent_dim': None, 'min_stdv': 100000, 'true_axes': None}\n",
      "self.methods,  ['pca', 'st']\n"
     ]
    }
   ],
   "source": [
    "N_PIXELS = 8\n",
    "\n",
    "problem = Image(num_pixels=N_PIXELS)\n",
    "\n",
    "util_func = AreaUtil() # would binarize the pixels by default\n",
    "# util_func = LargestRectangleUtil(image_shape=(N_PIXELS, N_PIXELS))\n",
    "# util_func = GradientAwareAreaUtil(penalty_param=0.5, image_shape=(N_PIXELS, N_PIXELS))\n",
    "\n",
    "exp = BopeExperiment(\n",
    "    problem = problem, \n",
    "    util_func=util_func, \n",
    "    methods=[\"st\",\"pca\"],\n",
    "    pe_strategies=[\"Random-f\"],\n",
    "    trial_idx = 31,\n",
    "    pca_var_threshold = 0.9,\n",
    "    output_path = \"/home/yz685/low_rank_BOPE/experiments/shapes/\" + \\\n",
    "        f\"{N_PIXELS}by{N_PIXELS}/\")\n",
    "\n",
    "\n",
    "exp.generate_random_experiment_data(n=256, compute_util = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = exp.Y\n",
    "train_comps = exp.comps\n",
    "\n",
    "pixel_bounds = torch.cat(\n",
    "    (torch.zeros(N_PIXELS*N_PIXELS).unsqueeze(0), torch.ones(N_PIXELS*N_PIXELS).unsqueeze(0)), \n",
    "    dim=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn PCA from train_Y\n",
    "\n",
    "# how to handle centering? \n",
    "# need centering when learning PCA\n",
    "# but when appplying it as input transform\n",
    "# what happens if we don't center?\n",
    "# seeems like the GP would just have a nonzero mean, but shouldn't change anything else\n",
    "\n",
    "def fit_pca(train_Y, var_threshold=0.9, weights = None):\n",
    "\n",
    "    # unweighted pca\n",
    "    if weights == None:\n",
    "        U, S, V = torch.svd(train_Y - train_Y.mean(dim=0))\n",
    "\n",
    "    # weighted pca\n",
    "    else:\n",
    "        assert weights.shape == (train_Y.shape[0], 1), \\\n",
    "            \"weights shape does not match train_Y shape\"\n",
    "        assert (weights >= 0).all(), \\\n",
    "            \"weights must be nonnegative\"\n",
    "            \n",
    "        weighted_mean = (weights * train_Y).sum(dim=0) / weights.sum(0)\n",
    "        train_Y_centered = train_Y - weighted_mean\n",
    "        U, S, V = torch.svd(weights * train_Y_centered)\n",
    "\n",
    "    S_squared = torch.square(S)\n",
    "    explained_variance = S_squared / S_squared.sum()\n",
    "\n",
    "    exceed_thres = (\n",
    "        np.cumsum(explained_variance) > 0.9\n",
    "    )\n",
    "    num_axes = len(exceed_thres) - sum(exceed_thres) + 1\n",
    "\n",
    "    pca_axes = torch.tensor(torch.transpose(V[:, : num_axes], -2, -1), dtype = torch.double)\n",
    "\n",
    "    return pca_axes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: have a function that calculates weights based on utility values / estimates\n",
    "# this can be put into src later\n",
    "\n",
    "def compute_weights(util_vals, weights_type, **kwargs):\n",
    "\n",
    "    # util_vals can be true utility values\n",
    "    # or predicted posterior mean from current utility model\n",
    "\n",
    "    # options for weights_type: rank_weighting as in Tripp et al. \n",
    "\n",
    "    if weights_type == \"rank\":\n",
    "        k = kwargs.get(\"k\", 10) # TODO: come back to this\n",
    "        utils_argsort = np.argsort(-np.asarray(util_vals))\n",
    "        ranks = np.argsort(utils_argsort)\n",
    "        weights = 1 / (k * len(util_vals) + ranks)\n",
    "\n",
    "    elif weights_type == \"power\":\n",
    "        pass\n",
    "        # rough idea: apply a power transformation to util_vals, make sure they are nonnegative\n",
    "\n",
    "    \n",
    "    return weights\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0233+0.j,  0.0634+0.j, -0.0186+0.j,  ...,  0.0705+0.j,  0.0120+0.j,\n",
       "         -0.0010+0.j],\n",
       "        [ 0.0517+0.j,  0.0938+0.j, -0.0367+0.j,  ...,  0.0334+0.j, -0.1840+0.j,\n",
       "          0.0549+0.j],\n",
       "        [ 0.0764+0.j,  0.0861+0.j, -0.0666+0.j,  ..., -0.0119+0.j,  0.0947+0.j,\n",
       "          0.0548+0.j],\n",
       "        ...,\n",
       "        [ 0.0518+0.j, -0.0997+0.j,  0.0738+0.j,  ...,  0.0068+0.j,  0.1174+0.j,\n",
       "          0.0220+0.j],\n",
       "        [ 0.0371+0.j, -0.0903+0.j,  0.0436+0.j,  ..., -0.0122+0.j,  0.0508+0.j,\n",
       "         -0.0954+0.j],\n",
       "        [ 0.0346+0.j, -0.0564+0.j,  0.0022+0.j,  ..., -0.0134+0.j, -0.1675+0.j,\n",
       "          0.1135+0.j]], dtype=torch.complex128)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.linalg.eig(torch.cov(train_Y.T))[1]\n",
    "# U, _, _ = torch.svd((train_Y - train_Y.mean(dim=0)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_util_model(train_Y, train_comps, input_transform):\n",
    "\n",
    "    util_model = PairwiseGP(\n",
    "        datapoints=train_Y, comparisons=train_comps, input_tf = input_transform)\n",
    "\n",
    "    mll_util = PairwiseLaplaceMarginalLogLikelihood(\n",
    "        util_model.likelihood, util_model)\n",
    "    fit_gpytorch_mll(mll_util)\n",
    "\n",
    "    return util_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 0\n",
      "tensor([40., 33.])\n",
      "round 1\n",
      "tensor([29., 28.])\n",
      "round 2\n",
      "tensor([33., 32.])\n",
      "round 3\n",
      "tensor([34., 38.])\n",
      "round 4\n",
      "tensor([27., 25.])\n",
      "round 5\n",
      "tensor([23., 26.])\n",
      "round 6\n",
      "tensor([30., 38.])\n",
      "round 7\n",
      "tensor([34., 23.])\n",
      "round 8\n",
      "tensor([38., 33.])\n",
      "round 9\n",
      "tensor([27., 39.])\n"
     ]
    }
   ],
   "source": [
    "# iteratively refit subspace, refit util model, and optimize EUBO\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    print(f'round {i}')\n",
    "\n",
    "    pca_axes = fit_pca(train_Y)\n",
    "    pca_input_transform = ChainedInputTransform(\n",
    "        **{\n",
    "            \"center\": InputCenter(N_PIXELS * N_PIXELS),\n",
    "            \"pca\": PCAInputTransform(axes=pca_axes),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    util_model = fit_util_model(train_Y=train_Y, train_comps=train_comps, input_transform=pca_input_transform)\n",
    "\n",
    "    acqf = AnalyticExpectedUtilityOfBestOption(\n",
    "        pref_model=util_model,\n",
    "    ).to(torch.double) \n",
    "\n",
    "    cand_Y, acqf_val = optimize_acqf(\n",
    "        acq_function=acqf,\n",
    "        q=2,\n",
    "        bounds=pixel_bounds,\n",
    "        num_restarts=16,\n",
    "        raw_samples=128,  \n",
    "        options={\"batch_limit\": 4, \"seed\": 0},\n",
    "    )\n",
    "\n",
    "    cand_utils = util_func(cand_Y)\n",
    "\n",
    "    cand_comps = gen_comps(cand_utils)\n",
    "    train_comps = torch.cat((train_comps, cand_comps + train_Y.shape[0]))\n",
    "    train_Y = torch.cat((train_Y, cand_Y))\n",
    "\n",
    "    print(cand_utils)\n",
    "\n",
    "\n",
    "# question: if pref_model is already operating on bounded outcome space, just with PCA as input transform, \n",
    "# wouldn't specifying the [0,1] box bounds already suffice? \n",
    "# so we just specify the bounds argument\n",
    "\n",
    "# this still has the issue of clipping to bounds in the pixel space,\n",
    "# later let's try computing the pc explicitly and applying inequality constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0bd83d5610>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(cand_Y[1].reshape(N_PIXELS, N_PIXELS)>0.5, cmap=plt.cm.gray_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6],\n",
       "        [8]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util_func(cand_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bope_pca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f178f7686bb85c5c6e141a85fd4c17c3082d63b89f6cfaecdf98c22c0047a219"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
