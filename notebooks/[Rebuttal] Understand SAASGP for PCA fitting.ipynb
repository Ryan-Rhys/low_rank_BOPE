{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "The goal of this notebook is to understand the efficacy of fitting SAASGP to individual principal components.\n",
    "\n",
    "Currently, we implement PCA-GP through an OutcomeTransform(), which performs the outcome transform and fits batch single-task GPs to the principal components.\n",
    "\n",
    "For problems with relatively high input dimensionality (e.g., lunar lander problem requires 12-dimensional input), it might be reasonable to impose some sparsity on the inputs. This can be done through fitting SAASGPs to the principal components.\n",
    "\n",
    "However, SAASGP does not currently support batch / multi-output fitting as the single-task GPs do. So making this change would be a lot of work.\n",
    "\n",
    "In this notebook I would like to understand how much value SAASGP can provide. Will take the following steps:\n",
    "- Use lunar lander as example, generate a dataset of input and outputs\n",
    "- Option 1: fit GP using PCAOutcomeTransform\n",
    "- Option 2: manually do PCA transform, fit individual saasgp models to each of the PCs\n",
    "- Diagnostic 1: Compare the outcome model MSE on a separate test set for option 1 vs option 2\n",
    "- Diagnostic 2: Are different input dimensions selected for different PCs? Our hypothesis is that they are different. Is this really the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import re\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from dataclasses import asdict, dataclass\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from typing import Any, Dict, List, NamedTuple\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "\n",
    "from low_rank_BOPE.src.lunar_lander import LunarLander\n",
    "from low_rank_BOPE.src.pref_learning_helpers import (\n",
    "    check_outcome_model_fit,\n",
    "    check_pref_model_fit,\n",
    "    find_max_posterior_mean,\n",
    "    fit_outcome_model,\n",
    "    fit_pref_model,\n",
    "    gen_exp_cand,\n",
    "    generate_random_exp_data,\n",
    "    generate_random_pref_data,\n",
    "    run_pref_learn\n",
    ")\n",
    "from low_rank_BOPE.src.transforms import (\n",
    "    generate_random_projection,\n",
    "    InputCenter,\n",
    "    LinearProjectionInputTransform,\n",
    "    LinearProjectionOutcomeTransform,\n",
    "    PCAInputTransform,\n",
    "    PCAOutcomeTransform,\n",
    "    SubsetOutcomeTransform,\n",
    ")\n",
    "from low_rank_BOPE.src.models import make_modified_kernel, MultitaskGPModel\n",
    "from low_rank_BOPE.src.diagnostics import (\n",
    "    empirical_max_outcome_error,\n",
    "    empirical_max_util_error,\n",
    "    mc_max_outcome_error,\n",
    "    mc_max_util_error,\n",
    ")\n",
    "from low_rank_BOPE.src.saasgp_utils import (\n",
    "    SaasPriorHelper,\n",
    "    add_saas_prior,\n",
    "    _get_map_saas_model,\n",
    "    get_fitted_map_saas_model,\n",
    "    get_and_fit_map_saas_model\n",
    ")\n",
    "\n",
    "from botorch.acquisition.objective import GenericMCObjective, LearnedObjective\n",
    "from botorch.fit import fit_gpytorch_mll, fit_gpytorch_model\n",
    "from botorch.optim.fit import fit_gpytorch_scipy\n",
    "from botorch.models.multitask import KroneckerMultiTaskGP\n",
    "from botorch.models.transforms.input import (\n",
    "    ChainedInputTransform,\n",
    "    FilterFeatures,\n",
    "    Normalize,\n",
    ")\n",
    "\n",
    "from botorch.models.transforms.outcome import ChainedOutcomeTransform, Standardize\n",
    "# from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "from botorch.sampling.samplers import SobolQMCNormalSampler\n",
    "from botorch.test_functions.base import MultiObjectiveTestProblem\n",
    "\n",
    "from gpytorch.kernels import LCMKernel, MaternKernel\n",
    "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
    "from gpytorch.priors import GammaPrior\n",
    "from gpytorch.priors.lkj_prior import LKJCovariancePrior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"initial_experimentation_batch\": 16,\n",
    "    \"n_check_post_mean\": 13,\n",
    "    \"every_n_comps\": 3,\n",
    "    \"outcome_dim\": 20\n",
    "}\n",
    "\n",
    "tkwargs = {'dtype': torch.double}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = LunarLander(num_envs=20)\n",
    "X, Y = generate_random_exp_data(problem, config[\"initial_experimentation_batch\"], batch_eval = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 12]), torch.Size([16, 20]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_pca_outcome_transform = ChainedOutcomeTransform(\n",
    "            **{\n",
    "                \"standardize\": Standardize(\n",
    "                    config[\"outcome_dim\"],\n",
    "                    min_stdv=100,  # TODO: try standardize again\n",
    "                ),\n",
    "                # \"pca\": PCAOutcomeTransform(num_axes=config[\"lin_proj_latent_dim\"]),\n",
    "                \"pca\": PCAOutcomeTransform(\n",
    "                    variance_explained_threshold=0.9\n",
    "                ),\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_model_pca = fit_outcome_model(\n",
    "    X,\n",
    "    Y,\n",
    "    outcome_transform=std_pca_outcome_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained variance:  tensor(0.9107, dtype=torch.float64)\n",
      "number of axes:  3\n"
     ]
    }
   ],
   "source": [
    "# 3 axes out of 20 explain 91% of variance\n",
    "print('explained variance: ', outcome_model_pca.outcome_transform['pca'].PCA_explained_variance)\n",
    "print('number of axes: ', outcome_model_pca.outcome_transform['pca'].num_axes.item())\n",
    "\n",
    "num_axes = outcome_model_pca.outcome_transform['pca'].num_axes.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qing's code for fitting individual saasgp's to the PCs\n",
    "\n",
    "Y_transformed = std_pca_outcome_transform(Y)[0]\n",
    "Xs = [X for _ in range(num_axes)]\n",
    "Ys = [Y_transformed[:, [i]] for i in range(num_axes)]\n",
    "Yvars = [\n",
    "    torch.full(Y_transformed[:, [i]].size(), torch.nan, **tkwargs)\n",
    "    for i in range(num_axes)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_model_pca_saas = get_and_fit_map_saas_model(\n",
    "    Xs = Xs,\n",
    "    Ys = Ys,\n",
    "    Yvars = Yvars,\n",
    "    task_features = [],\n",
    "    fidelity_features = [],\n",
    "    metric_names = []\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## next check posterior fitting error on outcomes (map back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 3])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome_model_pca_saas.posterior(X).mean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: the 3 comes from 3 principal components; 16 is because X has 16 data points; but where does the 4 come from??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also check what input dimensions are selected for each of the saasgps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000e+04, 1.0000e+04, 1.0000e+04, 1.0000e+04, 1.0000e-02,\n",
       "          1.0000e-02, 1.0000e+04, 1.0886e+02, 1.0000e+04, 1.0000e+04,\n",
       "          1.0000e+04, 1.0000e-02]],\n",
       "\n",
       "        [[1.0000e+04, 1.0000e+04, 1.0000e+04, 1.0000e+04, 1.0000e-02,\n",
       "          1.0000e-02, 1.0000e+04, 2.7298e+01, 1.0000e+04, 1.0000e+04,\n",
       "          1.0000e+04, 1.0000e-02]],\n",
       "\n",
       "        [[6.0955e+03, 1.0000e+04, 1.0000e+04, 1.0000e+04, 1.0000e-02,\n",
       "          1.0000e-02, 1.0000e+04, 1.0000e+04, 1.0000e+04, 9.8798e+03,\n",
       "          1.0000e+04, 1.0000e-02]],\n",
       "\n",
       "        [[9.8434e+03, 1.0000e+04, 1.0000e+04, 1.0000e+04, 1.0000e-02,\n",
       "          1.0000e-02, 1.0000e+04, 1.0000e+04, 1.0000e+04, 9.9988e+03,\n",
       "          1.0000e+04, 1.0000e-02]]], dtype=torch.float64,\n",
       "       grad_fn=<SoftplusBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome_model_pca_saas.models[0].covar_module.base_kernel.lengthscale\n",
    "# what does this imply?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f178f7686bb85c5c6e141a85fd4c17c3082d63b89f6cfaecdf98c22c0047a219"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('bope_pca': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
